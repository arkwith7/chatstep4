{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MemN으로 한국어 QA 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어 Babi 데이터셋에 대해서 메모리 네트워크를 테스트해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 커스터마이즈드 KoNLPy 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사실 위 문장에서 '은경이'는 사람 이름이므로 제대로 된 결과를 얻기 위해서는 '은', '경이'와 같이 글자가 분리되는 것이 아니라 '은경이' 또는 최소한 '은경'이라는 단어 토큰을 얻어야만 합니다. 이런 경우에는 형태소 분석기에 사용자 사전을 추가해줄 수 있습니다. '은경이'는 하나의 단어이기 때문에 분리하지말라고 형태소 분석기에 알려주는 것입니다.\n",
    "\n",
    "사용자 사전을 추가하는 방법은 형태소 분석기마다 다른데, 생각보다 복잡한 경우들이 많습니다. 이번 실습에서는 Customized Konlpy라는 사용자 사전 추가가 매우 쉬운 패키지를 사용합니다. 프롬프트에서 아래의 커맨드를 통해 형태소 분석기를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['은', '경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "twitter.morphs('은경이는 사무실로 갔습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter.add_dictionary('은경이', 'Noun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제대로 반영되었는지 동일한 예문을 다시 형태소 분석해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['은경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.morphs('은경이는 사무실로 갔습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제는 '은경이'라는 단어가 제대로 하나의 토큰으로 인식되는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 한국어 Babi 데이터셋 로드와 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어 Babi 데이터셋은 딥러닝을 이용한 자연어 처리 입문 저자가 영어 데이터셋을 참고하여 만들었으며, 아래의 링크에서 다운로드 할 수 있습니다.\n",
    "\n",
    "훈련 데이터 : https://bit.ly/31SqtHy\n",
    "테스트 데이터 : https://bit.ly/3f7rH5g\n",
    "\n",
    "우선 필요한 도구들을 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from functools import reduce\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = os.path.join(\"qa1_single-supporting-fact_train_kor.txt\")\n",
    "TEST_FILE = os.path.join(\"qa1_single-supporting-fact_test_kor.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터로부터 상위 20개의 문장을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 필웅이는 화장실로 갔습니다.\n",
      "2 은경이는 복도로 이동했습니다.\n",
      "3 필웅이는 어디야? \t화장실\t1\n",
      "4 수종이는 복도로 복귀했습니다.\n",
      "5 경임이는 정원으로 갔습니다.\n",
      "6 수종이는 어디야? \t복도\t4\n",
      "7 은경이는 사무실로 갔습니다.\n",
      "8 경임이는 화장실로 뛰어갔습니다.\n",
      "9 수종이는 어디야? \t복도\t4\n",
      "10 필웅이는 복도로 갔습니다.\n",
      "11 수종이는 사무실로 가버렸습니다.\n",
      "12 수종이는 어디야? \t사무실\t11\n",
      "13 은경이는 정원으로 복귀했습니다.\n",
      "14 은경이는 침실로 갔습니다.\n",
      "15 경임이는 어디야? \t화장실\t8\n",
      "1 경임이는 사무실로 가버렸습니다.\n",
      "2 경임이는 화장실로 이동했습니다.\n",
      "3 경임이는 어디야? \t화장실\t2\n",
      "4 필웅이는 침실로 이동했습니다.\n",
      "5 수종이는 복도로 갔습니다.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "lines = open(TRAIN_FILE , \"rb\")\n",
    "for line in lines:\n",
    "    line = line.decode(\"utf-8\").strip()\n",
    "    i = i + 1\n",
    "    print(line)\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어 Babi 데이터셋과 형식이 같은 것을 확인할 수 있습니다. read_data() 함수를 사용하여 훈련 데이터와 테스트 데이터를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(dir):\n",
    "    stories, questions, answers = [], [], [] # 각각 스토리, 질문, 답변을 저장할 예정\n",
    "    story_temp = [] # 현재 시점의 스토리 임시 저장\n",
    "    lines = open(dir, \"rb\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.decode(\"utf-8\") # b' 제거\n",
    "        line = line.strip() # '\\n' 제거\n",
    "        idx, text = line.split(\" \", 1) # 맨 앞에 있는 id number 분리\n",
    "        # 여기까지는 모든 줄에 적용되는 전처리\n",
    "\n",
    "        if int(idx) == 1:\n",
    "            story_temp = []\n",
    "\n",
    "        if \"\\t\" in text: # 현재 읽는 줄이 질문 (tab) 답변 (tab)인 경우\n",
    "            question, answer, _ = text.split(\"\\t\") # 질문과 답변을 각각 저장\n",
    "            stories.append([x for x in story_temp if x]) # 지금까지의 누적 스토리를 스토리에 저장\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "\n",
    "        else: # 현재 읽는 줄이 스토리인 경우\n",
    "            story_temp.append(text) # 임시 저장\n",
    "\n",
    "    lines.close()\n",
    "    return stories, questions, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def read_data(dir):\n",
    "\n",
    "  // 영어 데이터셋에 사용했던 이전 챕터의 read_data() 함수와 동일한 함수입니다.\n",
    "\n",
    "read_data() 함수는 영어 데이터셋을 사용한 이전 챕터의 실습에서 사용했던 함수와 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = read_data(TRAIN_FILE)\n",
    "test_data = read_data(TEST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터와 테스트 데이터가 로드되었습니다. 데이터의 구성을 확인하기 위해서 스토리, 질문, 답변을 각각 분리해서 로드해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stories, train_questions, train_answers = read_data(TRAIN_FILE)\n",
    "test_stories, test_questions, test_answers = read_data(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 스토리의 개수 : 10000\n",
      "훈련용 질문의 개수 : 10000\n",
      "훈련용 답변의 개수 : 10000\n",
      "테스트용 스토리의 개수 : 1000\n",
      "테스트용 질문의 개수 : 1000\n",
      "테스트용 답변의 개수 : 1000\n"
     ]
    }
   ],
   "source": [
    "print('훈련용 스토리의 개수 :', len(train_stories))\n",
    "print('훈련용 질문의 개수 :',len(train_questions))\n",
    "print('훈련용 답변의 개수 :',len(train_answers))\n",
    "print('테스트용 스토리의 개수 :',len(test_stories))\n",
    "print('테스트용 질문의 개수 :',len(test_questions))\n",
    "print('테스트용 답변의 개수 :',len(test_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의로 3573번째 스토리, 질문, 답변을 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['은경이는 부엌으로 가버렸습니다.',\n",
       " '필웅이는 사무실로 가버렸습니다.',\n",
       " '수종이는 복도로 뛰어갔습니다.',\n",
       " '은경이는 사무실로 복귀했습니다.',\n",
       " '경임이는 사무실로 이동했습니다.',\n",
       " '경임이는 침실로 갔습니다.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stories[3572]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'은경이는 어디야? '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions[3572]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'사무실'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answers[3572]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 토큰화 함수를 정의하고, 이로부터 Vocabulary를 생성하는 함수를 만들어봅시다. 아래의 함수는 이전 챕터에서 영어 데이터셋에 사용했던 토큰화 함수와 동일합니다. 현재는 한국어이므로 아래의 토큰화 함수를 그대로 사용하는 것은 바람직하지는 않지만, 임시로 사용해보겠습니다. 어절 단위로 했을 때 어떤 단어들이 있는지 출력해보기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return [ x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Vocabulary를 만드는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data):\n",
    "    counter = FreqDist()\n",
    "\n",
    "    # 두 문장의 story를 하나의 문장으로 통합하는 함수\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    # 각 샘플의 길이를 저장하는 리스트\n",
    "    story_len = []\n",
    "    question_len = []\n",
    "\n",
    "    for stories, questions, answers in [train_data, test_data]:\n",
    "        for story in stories:\n",
    "            stories = tokenize(flatten(story)) # 스토리의 문장들을 펼친 후 토큰화\n",
    "            story_len.append(len(stories)) # 각 story의 길이 저장\n",
    "            for word in stories: # 단어 집합에 단어 추가\n",
    "                counter[word] += 1\n",
    "        for question in questions:\n",
    "            question = tokenize(question)\n",
    "            question_len.append(len(question))\n",
    "            for word in question:\n",
    "                counter[word] += 1\n",
    "        for answer in answers:\n",
    "            answer = tokenize(answer)\n",
    "            for word in answer:\n",
    "                counter[word] += 1\n",
    "\n",
    "    # 단어 집합 생성\n",
    "    word2idx = {word : (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}\n",
    "    idx2word = {idx : word for word, idx in word2idx.items()}\n",
    "\n",
    "    # 가장 긴 샘플의 길이\n",
    "    story_max_len = np.max(story_len)\n",
    "    question_max_len = np.max(question_len)\n",
    "\n",
    "    return word2idx, idx2word, story_max_len, question_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 1, '경임이는': 2, '은경이는': 3, '수종이는': 4, '필웅이는': 5, '이동했습니다': 6, '가버렸습니다': 7, '뛰어갔습니다': 8, '복귀했습니다': 9, '갔습니다': 10, '화장실로': 11, '정원으로': 12, '복도로': 13, '어디야': 14, '?': 15, '부엌으로': 16, '사무실로': 17, '침실로': 18, '화장실': 19, '정원': 20, '사무실': 21, '침실': 22, '복도': 23, '부엌': 24}\n"
     ]
    }
   ],
   "source": [
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스토리의 최대 길이 : 40\n",
      "질문의 최대 길이 : 3\n"
     ]
    }
   ],
   "source": [
    "print('스토리의 최대 길이 :',story_max_len)\n",
    "print('질문의 최대 길이 :',question_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(data, word2idx, story_maxlen, question_maxlen):\n",
    "    Xs, Xq, Y = [], [], []\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    stories, questions, answers = data\n",
    "    for story, question, answer in zip(stories, questions, answers):\n",
    "        xs = [word2idx[w] for w in tokenize(flatten(story))]\n",
    "        xq = [word2idx[w] for w in tokenize(question)]\n",
    "        Xs.append(xs)\n",
    "        Xq.append(xq)\n",
    "        Y.append(word2idx[answer])\n",
    "\n",
    "        # 스토리와 질문은 각각의 최대 길이로 패딩\n",
    "        # 정답은 원-핫 인코딩\n",
    "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
    "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
    "           to_categorical(Y, num_classes=len(word2idx) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xstrain, Xqtrain, Ytrain = vectorize(train_data, word2idx, story_max_len, question_max_len)\n",
    "Xstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 40) (10000, 3) (10000, 25) (1000, 40) (1000, 3) (1000, 25)\n"
     ]
    }
   ],
   "source": [
    "print(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 메모리 네트워크로 QA 태스크 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Permute, dot, add, concatenate\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 에포크 횟수\n",
    "train_epochs = 120\n",
    "# 배치 크기\n",
    "batch_size = 32\n",
    "# 임베딩 크기\n",
    "embed_size = 50\n",
    "# LSTM의 크기\n",
    "lstm_size = 64\n",
    "# 과적합 방지 기법인 드롭아웃 적용 비율\n",
    "dropout_rate = 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stories : Tensor(\"input_1:0\", shape=(?, 40), dtype=float32)\n",
      "Question: Tensor(\"input_2:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 플레이스 홀더. 입력을 담는 변수\n",
    "input_sequence = Input((story_max_len,))\n",
    "question = Input((question_max_len,))\n",
    "\n",
    "print('Stories :', input_sequence)\n",
    "print('Question:', question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 스토리를 위한 첫번째 임베딩. 그림에서의 Embedding A\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embed_size))\n",
    "input_encoder_m.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, embedding_dim) / 샘플의 수, 문장의 최대 길이, 임베딩 벡터의 차원\n",
    "\n",
    "# 스토리를 위한 두번째 임베딩. 그림에서의 Embedding C\n",
    "# 임베딩 벡터의 차원을 question_max_len(질문의 최대 길이)로 한다.\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=question_max_len))\n",
    "input_encoder_c.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 질문을 위한 임베딩. 그림에서의 Embedding B\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=embed_size,\n",
    "                               input_length=question_max_len))\n",
    "question_encoder.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, question_max_len, embedding_dim) / 샘플의 수, 질문의 최대 길이, 임베딩 벡터의 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input encoded m Tensor(\"sequential/dropout/cond/Merge:0\", shape=(?, 40, 50), dtype=float32)\n",
      "Input encoded c Tensor(\"sequential_1/dropout_1/cond/Merge:0\", shape=(?, 40, 3), dtype=float32)\n",
      "Question encoded Tensor(\"sequential_2/dropout_2/cond/Merge:0\", shape=(?, 3, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 실질적인 임베딩 과정\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "print('Input encoded m', input_encoded_m)\n",
    "print('Input encoded c', input_encoded_c)\n",
    "print('Question encoded', question_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match shape Tensor(\"activation/truediv:0\", shape=(?, 40, 3), dtype=float32)\n",
      "Response shape Tensor(\"permute/transpose:0\", shape=(?, 3, 40), dtype=float32)\n",
      "Answer shape Tensor(\"concatenate/concat:0\", shape=(?, 3, 90), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\n",
    "# 유사도는 내적을 사용한다.\n",
    "match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\n",
    "match = Activation('softmax')(match)\n",
    "print('Match shape', match)\n",
    "# 결과 : (samples, story_maxlen, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_max_len, question_max_len)\n",
    "response = Permute((2, 1))(response)  # (samples, question_max_len, story_max_len)\n",
    "print('Response shape', response)\n",
    "\n",
    "# concatenate the response vector with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "print('Answer shape', answer)\n",
    "\n",
    "answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n",
    "answer = Dropout(dropout_rate)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         multiple             1250        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 3, 50)        1250        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 40, 3)        0           sequential[1][0]                 \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 40, 3)        0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       multiple             75          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 40, 3)        0           activation[0][0]                 \n",
      "                                                                 sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 3, 40)        0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3, 90)        0           permute[0][0]                    \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           39680       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 25)           1625        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 25)           0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 43,880\n",
      "Trainable params: 43,880\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 10000 samples, validate on 1000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 2s 248us/sample - loss: 1.8974 - acc: 0.1708 - val_loss: 1.7938 - val_acc: 0.1870\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 2s 172us/sample - loss: 1.7214 - acc: 0.2487 - val_loss: 1.6000 - val_acc: 0.3760\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 2s 164us/sample - loss: 1.5403 - acc: 0.3803 - val_loss: 1.4663 - val_acc: 0.4180\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 2s 166us/sample - loss: 1.4859 - acc: 0.4072 - val_loss: 1.4204 - val_acc: 0.4420\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 2s 165us/sample - loss: 1.4268 - acc: 0.4454 - val_loss: 1.3404 - val_acc: 0.4750\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 2s 166us/sample - loss: 1.3760 - acc: 0.4596 - val_loss: 1.3306 - val_acc: 0.4930\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 2s 166us/sample - loss: 1.3570 - acc: 0.4645 - val_loss: 1.3027 - val_acc: 0.5010\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 2s 170us/sample - loss: 1.3417 - acc: 0.4676 - val_loss: 1.2907 - val_acc: 0.5000\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 2s 166us/sample - loss: 1.3289 - acc: 0.4808 - val_loss: 1.2708 - val_acc: 0.4980\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 2s 185us/sample - loss: 1.3126 - acc: 0.4786 - val_loss: 1.2638 - val_acc: 0.5010\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 2s 173us/sample - loss: 1.2975 - acc: 0.4933 - val_loss: 1.2570 - val_acc: 0.5020\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 1.2866 - acc: 0.4999 - val_loss: 1.2296 - val_acc: 0.5110\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 2s 184us/sample - loss: 1.2676 - acc: 0.5020 - val_loss: 1.2191 - val_acc: 0.5160\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 2s 176us/sample - loss: 1.2554 - acc: 0.5033 - val_loss: 1.2340 - val_acc: 0.5140\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 2s 164us/sample - loss: 1.2432 - acc: 0.5089 - val_loss: 1.2379 - val_acc: 0.4900\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 1.2283 - acc: 0.5158 - val_loss: 1.1955 - val_acc: 0.5160\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 1.2165 - acc: 0.5173 - val_loss: 1.1754 - val_acc: 0.5330\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 1.1943 - acc: 0.5342 - val_loss: 1.1502 - val_acc: 0.5590\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 1.1593 - acc: 0.5617 - val_loss: 1.1081 - val_acc: 0.5850\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 2s 162us/sample - loss: 1.1036 - acc: 0.5801 - val_loss: 1.0385 - val_acc: 0.6130\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 2s 157us/sample - loss: 1.0311 - acc: 0.6190 - val_loss: 0.9317 - val_acc: 0.6710\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 2s 169us/sample - loss: 0.9536 - acc: 0.6567 - val_loss: 0.8610 - val_acc: 0.7030\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 0.8688 - acc: 0.6876 - val_loss: 0.7633 - val_acc: 0.7330\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 0.8156 - acc: 0.7071 - val_loss: 0.7282 - val_acc: 0.7300\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 2s 189us/sample - loss: 0.7582 - acc: 0.7250 - val_loss: 0.7140 - val_acc: 0.7320\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 2s 185us/sample - loss: 0.7360 - acc: 0.7339 - val_loss: 0.6857 - val_acc: 0.7510\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 2s 181us/sample - loss: 0.7008 - acc: 0.7419 - val_loss: 0.6754 - val_acc: 0.7550\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 2s 178us/sample - loss: 0.6614 - acc: 0.7560 - val_loss: 0.6383 - val_acc: 0.7490\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 2s 179us/sample - loss: 0.6425 - acc: 0.7645 - val_loss: 0.6416 - val_acc: 0.7560\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 2s 172us/sample - loss: 0.6163 - acc: 0.7732 - val_loss: 0.6123 - val_acc: 0.7740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 2s 169us/sample - loss: 0.5901 - acc: 0.7825 - val_loss: 0.5834 - val_acc: 0.7870\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 2s 177us/sample - loss: 0.5574 - acc: 0.7962 - val_loss: 0.5651 - val_acc: 0.7910\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 2s 173us/sample - loss: 0.5389 - acc: 0.8042 - val_loss: 0.5556 - val_acc: 0.7970\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 2s 167us/sample - loss: 0.5138 - acc: 0.8099 - val_loss: 0.5124 - val_acc: 0.8170\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 2s 183us/sample - loss: 0.4955 - acc: 0.8148 - val_loss: 0.4820 - val_acc: 0.8280\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 2s 182us/sample - loss: 0.4657 - acc: 0.8301 - val_loss: 0.4659 - val_acc: 0.8350\n",
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 2s 181us/sample - loss: 0.4439 - acc: 0.8349 - val_loss: 0.4491 - val_acc: 0.8330\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 2s 176us/sample - loss: 0.4276 - acc: 0.8430 - val_loss: 0.4297 - val_acc: 0.8440\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 2s 168us/sample - loss: 0.4022 - acc: 0.8504 - val_loss: 0.4139 - val_acc: 0.8460\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 2s 169us/sample - loss: 0.3933 - acc: 0.8507 - val_loss: 0.3931 - val_acc: 0.8500\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 2s 167us/sample - loss: 0.3751 - acc: 0.8615 - val_loss: 0.4033 - val_acc: 0.8580\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 2s 166us/sample - loss: 0.3678 - acc: 0.8621 - val_loss: 0.3862 - val_acc: 0.8560\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 0.3492 - acc: 0.8707 - val_loss: 0.3773 - val_acc: 0.8630\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 2s 164us/sample - loss: 0.3486 - acc: 0.8687 - val_loss: 0.3770 - val_acc: 0.8670\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 2s 163us/sample - loss: 0.3366 - acc: 0.8731 - val_loss: 0.3662 - val_acc: 0.8680\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 2s 163us/sample - loss: 0.3264 - acc: 0.8766 - val_loss: 0.3723 - val_acc: 0.8620\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 2s 162us/sample - loss: 0.3189 - acc: 0.8819 - val_loss: 0.3637 - val_acc: 0.8580\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 2s 163us/sample - loss: 0.3165 - acc: 0.8799 - val_loss: 0.3602 - val_acc: 0.8680\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 2s 166us/sample - loss: 0.3062 - acc: 0.8850 - val_loss: 0.3655 - val_acc: 0.8610\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 2s 172us/sample - loss: 0.2968 - acc: 0.8907 - val_loss: 0.3458 - val_acc: 0.8690\n",
      "Epoch 51/120\n",
      "10000/10000 [==============================] - 2s 179us/sample - loss: 0.2935 - acc: 0.8914 - val_loss: 0.3444 - val_acc: 0.8690\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 2s 162us/sample - loss: 0.2798 - acc: 0.8971 - val_loss: 0.3326 - val_acc: 0.8780\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 2s 180us/sample - loss: 0.2796 - acc: 0.8979 - val_loss: 0.3290 - val_acc: 0.8710\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 2s 182us/sample - loss: 0.2696 - acc: 0.9014 - val_loss: 0.3178 - val_acc: 0.8740\n",
      "Epoch 55/120\n",
      "10000/10000 [==============================] - 2s 171us/sample - loss: 0.2620 - acc: 0.9039 - val_loss: 0.3227 - val_acc: 0.8730\n",
      "Epoch 56/120\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 0.2569 - acc: 0.9051 - val_loss: 0.2994 - val_acc: 0.8830\n",
      "Epoch 57/120\n",
      "10000/10000 [==============================] - 2s 165us/sample - loss: 0.2536 - acc: 0.9070 - val_loss: 0.3017 - val_acc: 0.8890\n",
      "Epoch 58/120\n",
      "10000/10000 [==============================] - 2s 173us/sample - loss: 0.2445 - acc: 0.9099 - val_loss: 0.2985 - val_acc: 0.8860\n",
      "Epoch 59/120\n",
      "10000/10000 [==============================] - 2s 165us/sample - loss: 0.2356 - acc: 0.9142 - val_loss: 0.2865 - val_acc: 0.8930\n",
      "Epoch 60/120\n",
      "10000/10000 [==============================] - 2s 159us/sample - loss: 0.2325 - acc: 0.9150 - val_loss: 0.3065 - val_acc: 0.8860\n",
      "Epoch 61/120\n",
      "10000/10000 [==============================] - 2s 163us/sample - loss: 0.2164 - acc: 0.9231 - val_loss: 0.2663 - val_acc: 0.8940\n",
      "Epoch 62/120\n",
      "10000/10000 [==============================] - 2s 169us/sample - loss: 0.2161 - acc: 0.9221 - val_loss: 0.2728 - val_acc: 0.8920\n",
      "Epoch 63/120\n",
      "10000/10000 [==============================] - 2s 164us/sample - loss: 0.2155 - acc: 0.9254 - val_loss: 0.2638 - val_acc: 0.8980\n",
      "Epoch 64/120\n",
      "10000/10000 [==============================] - 2s 167us/sample - loss: 0.2093 - acc: 0.9240 - val_loss: 0.2660 - val_acc: 0.8960\n",
      "Epoch 65/120\n",
      "10000/10000 [==============================] - 2s 171us/sample - loss: 0.1965 - acc: 0.9282 - val_loss: 0.2902 - val_acc: 0.8920\n",
      "Epoch 66/120\n",
      "10000/10000 [==============================] - 2s 189us/sample - loss: 0.1985 - acc: 0.9293 - val_loss: 0.2693 - val_acc: 0.8980\n",
      "Epoch 67/120\n",
      "10000/10000 [==============================] - 2s 172us/sample - loss: 0.1918 - acc: 0.9290 - val_loss: 0.2676 - val_acc: 0.8920\n",
      "Epoch 68/120\n",
      "10000/10000 [==============================] - 2s 171us/sample - loss: 0.1820 - acc: 0.9337 - val_loss: 0.2676 - val_acc: 0.9020\n",
      "Epoch 69/120\n",
      "10000/10000 [==============================] - 2s 163us/sample - loss: 0.1845 - acc: 0.9332 - val_loss: 0.2475 - val_acc: 0.9000\n",
      "Epoch 70/120\n",
      "10000/10000 [==============================] - 2s 170us/sample - loss: 0.1777 - acc: 0.9395 - val_loss: 0.2548 - val_acc: 0.9020\n",
      "Epoch 71/120\n",
      "10000/10000 [==============================] - 2s 171us/sample - loss: 0.1762 - acc: 0.9351 - val_loss: 0.2553 - val_acc: 0.8980\n",
      "Epoch 72/120\n",
      "10000/10000 [==============================] - 2s 167us/sample - loss: 0.1706 - acc: 0.9388 - val_loss: 0.2498 - val_acc: 0.9020\n",
      "Epoch 73/120\n",
      "10000/10000 [==============================] - 2s 169us/sample - loss: 0.1631 - acc: 0.9422 - val_loss: 0.2591 - val_acc: 0.9020\n",
      "Epoch 74/120\n",
      "10000/10000 [==============================] - 2s 185us/sample - loss: 0.1620 - acc: 0.9438 - val_loss: 0.2607 - val_acc: 0.9100\n",
      "Epoch 75/120\n",
      "10000/10000 [==============================] - 2s 172us/sample - loss: 0.1615 - acc: 0.9423 - val_loss: 0.2572 - val_acc: 0.9050\n",
      "Epoch 76/120\n",
      "10000/10000 [==============================] - 2s 166us/sample - loss: 0.1528 - acc: 0.9450 - val_loss: 0.2569 - val_acc: 0.9100\n",
      "Epoch 77/120\n",
      "10000/10000 [==============================] - 2s 185us/sample - loss: 0.1531 - acc: 0.9453 - val_loss: 0.2441 - val_acc: 0.9110\n",
      "Epoch 78/120\n",
      "10000/10000 [==============================] - 2s 169us/sample - loss: 0.1507 - acc: 0.9475 - val_loss: 0.2446 - val_acc: 0.9100\n",
      "Epoch 79/120\n",
      "10000/10000 [==============================] - 2s 167us/sample - loss: 0.1465 - acc: 0.9473 - val_loss: 0.2511 - val_acc: 0.9130\n",
      "Epoch 80/120\n",
      "10000/10000 [==============================] - 2s 164us/sample - loss: 0.1373 - acc: 0.9510 - val_loss: 0.2501 - val_acc: 0.9170\n",
      "Epoch 81/120\n",
      "10000/10000 [==============================] - 2s 169us/sample - loss: 0.1364 - acc: 0.9525 - val_loss: 0.2566 - val_acc: 0.9080\n",
      "Epoch 82/120\n",
      "10000/10000 [==============================] - 2s 164us/sample - loss: 0.1361 - acc: 0.9535 - val_loss: 0.2392 - val_acc: 0.9140\n",
      "Epoch 83/120\n",
      "10000/10000 [==============================] - 2s 166us/sample - loss: 0.1385 - acc: 0.9531 - val_loss: 0.2387 - val_acc: 0.9170\n",
      "Epoch 84/120\n",
      "10000/10000 [==============================] - 2s 162us/sample - loss: 0.1240 - acc: 0.9549 - val_loss: 0.2569 - val_acc: 0.9150\n",
      "Epoch 85/120\n",
      "10000/10000 [==============================] - 2s 168us/sample - loss: 0.1303 - acc: 0.9531 - val_loss: 0.2552 - val_acc: 0.9140\n",
      "Epoch 86/120\n",
      "10000/10000 [==============================] - 2s 172us/sample - loss: 0.1251 - acc: 0.9541 - val_loss: 0.2330 - val_acc: 0.9230\n",
      "Epoch 87/120\n",
      "10000/10000 [==============================] - 2s 172us/sample - loss: 0.1258 - acc: 0.9553 - val_loss: 0.2525 - val_acc: 0.9180\n",
      "Epoch 88/120\n",
      "10000/10000 [==============================] - 2s 174us/sample - loss: 0.1125 - acc: 0.9578 - val_loss: 0.2569 - val_acc: 0.9150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/120\n",
      "10000/10000 [==============================] - 2s 160us/sample - loss: 0.1151 - acc: 0.9598 - val_loss: 0.2658 - val_acc: 0.9110\n",
      "Epoch 90/120\n",
      "10000/10000 [==============================] - 2s 163us/sample - loss: 0.1120 - acc: 0.9630 - val_loss: 0.2432 - val_acc: 0.9190\n",
      "Epoch 91/120\n",
      "10000/10000 [==============================] - 2s 162us/sample - loss: 0.1081 - acc: 0.9617 - val_loss: 0.2423 - val_acc: 0.9220\n",
      "Epoch 92/120\n",
      "10000/10000 [==============================] - 2s 177us/sample - loss: 0.1098 - acc: 0.9623 - val_loss: 0.2677 - val_acc: 0.9180\n",
      "Epoch 93/120\n",
      "10000/10000 [==============================] - 2s 168us/sample - loss: 0.1032 - acc: 0.9623 - val_loss: 0.2491 - val_acc: 0.9190\n",
      "Epoch 94/120\n",
      "10000/10000 [==============================] - 2s 165us/sample - loss: 0.1028 - acc: 0.9630 - val_loss: 0.2697 - val_acc: 0.9090\n",
      "Epoch 95/120\n",
      "10000/10000 [==============================] - 2s 175us/sample - loss: 0.1002 - acc: 0.9661 - val_loss: 0.2604 - val_acc: 0.9210\n",
      "Epoch 96/120\n",
      "10000/10000 [==============================] - 2s 186us/sample - loss: 0.1009 - acc: 0.9648 - val_loss: 0.2493 - val_acc: 0.9170\n",
      "Epoch 97/120\n",
      "10000/10000 [==============================] - 2s 170us/sample - loss: 0.0910 - acc: 0.9665 - val_loss: 0.2532 - val_acc: 0.9270\n",
      "Epoch 98/120\n",
      "10000/10000 [==============================] - 2s 179us/sample - loss: 0.0972 - acc: 0.9669 - val_loss: 0.2480 - val_acc: 0.9230\n",
      "Epoch 99/120\n",
      "10000/10000 [==============================] - 2s 166us/sample - loss: 0.0928 - acc: 0.9676 - val_loss: 0.2461 - val_acc: 0.9230\n",
      "Epoch 100/120\n",
      "10000/10000 [==============================] - 2s 173us/sample - loss: 0.0934 - acc: 0.9684 - val_loss: 0.2457 - val_acc: 0.9270\n",
      "Epoch 101/120\n",
      "10000/10000 [==============================] - 2s 172us/sample - loss: 0.0882 - acc: 0.9682 - val_loss: 0.2454 - val_acc: 0.9210\n",
      "Epoch 102/120\n",
      "10000/10000 [==============================] - 2s 167us/sample - loss: 0.0835 - acc: 0.9697 - val_loss: 0.2458 - val_acc: 0.9270\n",
      "Epoch 103/120\n",
      "10000/10000 [==============================] - 2s 168us/sample - loss: 0.0853 - acc: 0.9700 - val_loss: 0.2527 - val_acc: 0.9260\n",
      "Epoch 104/120\n",
      "10000/10000 [==============================] - 2s 171us/sample - loss: 0.0817 - acc: 0.9724 - val_loss: 0.2555 - val_acc: 0.9280\n",
      "Epoch 105/120\n",
      "10000/10000 [==============================] - 2s 171us/sample - loss: 0.0853 - acc: 0.9707 - val_loss: 0.2489 - val_acc: 0.9230\n",
      "Epoch 106/120\n",
      "10000/10000 [==============================] - 2s 177us/sample - loss: 0.0784 - acc: 0.9712 - val_loss: 0.2356 - val_acc: 0.9310\n",
      "Epoch 107/120\n",
      "10000/10000 [==============================] - 2s 180us/sample - loss: 0.0790 - acc: 0.9721 - val_loss: 0.2516 - val_acc: 0.9300\n",
      "Epoch 108/120\n",
      "10000/10000 [==============================] - 2s 164us/sample - loss: 0.0758 - acc: 0.9735 - val_loss: 0.2408 - val_acc: 0.9270\n",
      "Epoch 109/120\n",
      "10000/10000 [==============================] - 2s 166us/sample - loss: 0.0776 - acc: 0.9715 - val_loss: 0.2494 - val_acc: 0.9300\n",
      "Epoch 110/120\n",
      "10000/10000 [==============================] - 2s 166us/sample - loss: 0.0775 - acc: 0.9717 - val_loss: 0.2469 - val_acc: 0.9270\n",
      "Epoch 111/120\n",
      "10000/10000 [==============================] - 2s 165us/sample - loss: 0.0731 - acc: 0.9736 - val_loss: 0.2526 - val_acc: 0.9280\n",
      "Epoch 112/120\n",
      "10000/10000 [==============================] - 2s 168us/sample - loss: 0.0724 - acc: 0.9746 - val_loss: 0.2387 - val_acc: 0.9310\n",
      "Epoch 113/120\n",
      "10000/10000 [==============================] - 2s 163us/sample - loss: 0.0737 - acc: 0.9755 - val_loss: 0.2323 - val_acc: 0.9360\n",
      "Epoch 114/120\n",
      "10000/10000 [==============================] - 2s 168us/sample - loss: 0.0655 - acc: 0.9766 - val_loss: 0.2514 - val_acc: 0.9360\n",
      "Epoch 115/120\n",
      "10000/10000 [==============================] - 2s 163us/sample - loss: 0.0649 - acc: 0.9777 - val_loss: 0.2508 - val_acc: 0.9290\n",
      "Epoch 116/120\n",
      "10000/10000 [==============================] - 2s 168us/sample - loss: 0.0686 - acc: 0.9740 - val_loss: 0.2552 - val_acc: 0.9300\n",
      "Epoch 117/120\n",
      "10000/10000 [==============================] - 2s 163us/sample - loss: 0.0643 - acc: 0.9773 - val_loss: 0.2541 - val_acc: 0.9320\n",
      "Epoch 118/120\n",
      "10000/10000 [==============================] - 2s 170us/sample - loss: 0.0598 - acc: 0.9798 - val_loss: 0.2694 - val_acc: 0.9310\n",
      "Epoch 119/120\n",
      "10000/10000 [==============================] - 2s 164us/sample - loss: 0.0641 - acc: 0.9779 - val_loss: 0.2524 - val_acc: 0.9340\n",
      "Epoch 120/120\n",
      "10000/10000 [==============================] - 2s 168us/sample - loss: 0.0637 - acc: 0.9778 - val_loss: 0.2536 - val_acc: 0.9340\n"
     ]
    }
   ],
   "source": [
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# start training the model\n",
    "history = model.fit([Xstrain, Xqtrain],\n",
    "         Ytrain, batch_size, train_epochs,\n",
    "         validation_data=([Xstest, Xqtest], Ytest))\n",
    "\n",
    "# save model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 64us/sample - loss: 0.2536 - acc: 0.9340\n",
      "\n",
      " 테스트 정확도: 0.9340\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate([Xstest, Xqtest], Ytest)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABMaUlEQVR4nO3deXiU1dn48e89k8lk3wOEBAj7HraAUQShqAUVtZYqLq3WrdXaaqt1qdpq39ra1778tIu1at0qgi2uVcQVRRSQRYSwbwFCWLJnkkzWOb8/ziQETCBIkkmG+3Ndc2XmWe8zhOfOOc95zhFjDEoppVRn4wh0AEoppVRzNEEppZTqlDRBKaWU6pQ0QSmllOqUNEEppZTqlDRBKaWU6pQ0QSmllOqUNEEp1Uoi8rGIFIuIO9CxKHUq0ASlVCuISDowCTDAhR143pCOOpdSnY0mKKVa5wfAcuA54OqGhSLSS0ReFZF8ESkUkb82WXeDiGwSEY+IbBSRsf7lRkQGNNnuORH5nf/9FBHJFZG7ROQA8KyIxIvIW/5zFPvfpzXZP0FEnhWRPP/61/3Ls0VkZpPtXCJSICKj2+k7UqpNaYJSqnV+AMz1v74tIt1FxAm8BewG0oFUYD6AiHwPeMC/Xwy21lXYynP1ABKAPsCN2P+nz/o/9wa8wF+bbP8vIAIYDnQD/p9/+QvAVU22Ow/Yb4xZ28o4lAoo0bH4lDo2ETkTWAykGGMKRGQz8A9sjepN//K6o/Z5F1hojHmsmeMZYKAxZrv/83NArjHmPhGZArwHxBhjqlqIZzSw2BgTLyIpwD4g0RhTfNR2PYEtQKoxpkxEFgBfGGP+9xt+FUp1KK1BKXV8VwPvGWMK/J9f8i/rBew+Ojn59QJ2fMPz5TdNTiISISL/EJHdIlIGLAHi/DW4XkDR0ckJwBiTB3wGfFdE4oAZ2BqgUl2C3oBV6hhEJBy4FHD67wkBuIE44CDQW0RCmklSe4H+LRy2Etsk16AHkNvk89HNGrcDg4HTjDEH/DWoLwHxnydBROKMMSXNnOt54Hrs//Vlxph9LcSkVKejNSilju1ioB4YBoz2v4YCn/rX7QceFpFIEQkTkYn+/Z4G7hCRcWINEJE+/nVrgStExCki04GzjhNDNPa+U4mIJAC/aVhhjNkPvAM87u9M4RKRyU32fR0YC9yKvSelVJehCUqpY7saeNYYs8cYc6Dhhe2kcDkwExgA7MHWgi4DMMb8B3gI2xzowSaKBP8xb/XvVwJc6V93LI8C4UAB9r7XoqPWfx+oBTYDh4DbGlYYY7zAK0Bf4NXWF1upwNNOEkoFORH5NTDIGHPVcTdWqhPRe1BKBTF/k+B12FqWUl2KNvEpFaRE5AZsJ4p3jDFLAh2PUidKm/iUUkp1SsetQYnIMyJySESyW1gvIvJnEdkuIusahnPxr5suIlv86+5uy8CVUkoFt+PWoPxdVsuBF4wxI5pZfx7wU+wwKqcBjxljTvM/RLgVOAfbu2klcLkxZuPxgkpKSjLp6eknWBSllFJd0erVqwuMMclHLz9uJwljzBL/SM4tuQibvAywXETi/MOvpAPbjTE7AURkvn/b4yao9PR0Vq1adbzNlFJKBQER2d3c8rboJJGKvRHbINe/rKXlSiml1HG1RYKSZpaZYyxv/iAiN4rIKhFZlZ+f3wZhKaWUOlnGGKrrqglEh7q2eA4qFztgZYM0IA8IbWF5s4wxTwJPAmRmZmrXQqVU0DLGUFpdysHyg+RX5uN2uokLiyM2LBZjDLW+Wmrra6mqq6KqrgpvnRdPtQdPjYfqumriw+NJCE8gxh2DMQaf8VFVV0WRt4jiqmLKqsuorK3EW+vFW+e1x/C/r6ytpKK2gsraysZtkiOT6R/fn96xvSn2FrO7dDd7SvdwoPwAB8oP4K3zEuoMJT4sntiwWBziQBBEhDU3rsEd0j6TTLdFgnoTuMV/j+k0oNQYs19E8oGBItIXOx3AbOCKNjifUkodkzGG/Mp89pTuoaa+hqjQKKJCoxCEmvoaan21jTUCg6G2vpbq+mq8tV7yPHnsLdvLwfKDuEPcRIVG4Xa68dR4KKkqwVPjoc5XR52vjtr6Wmrqa6iur6a6rprq+mqq6qqo99XjEAdOh5PqumrKqssoqy6jqq4Kn/FRb+o79PsIDwnHHeImwhXR+Ip0RRIZGklCeAIHyg+wct9KiquKCXWG0ju2N71ienFGrzPoHtmdhPAEPDUeirxFlFWX4TM+DAZjDA5pv8dpj5ugRGQeMAVIEpFc7ECVLgBjzBPAQmwPvu3YUZp/6F9XJyK3AO8CTuAZY8yGdiiDUqqLqPPV4an2UFZdRkVtBd5a+9e9iOAUJw5xUFZdRnFVMeU15USFRhEfFk9UaFRjAvHWeSmvKaeipoKSqhIOVhzkYMVBDlUcoshbRGFlIQfKD1BdX31SsUaHRlPrs7UYAIc4iAuLIzo0GpfTRYgjBKc4cYe4cTvdjTWMsJAwnA4nxhjqTT1up5sYdwzRodGEhYQ1Jq64sDi6R3YnKSKJWl8tJVUllFaVIiK4HC5cThdhIWGEh4QTFhJGtDua6NBoQp2hlFSVUOgtxFPtwSEOHOIg1BlKQnhCY80qwhVBuCsct9ONSHN3XL6uvKacCFdEuyadE9EpH9TNzMw02otPqcDy1nrtRbO6FG+tl6jQKGLcMTgdTg6UH2C/Zz/FVcXU+eqo99VTWVtJfmU+BZUFeGu9thlIhILKAnYU72Bn8U5KqkraPM4IVwQ9onrQLbIbieGJJIQn0D2yO33i+tArphfhrnDKa8rxVHsAcDlduByuIy7Coc5QQp2hhIWEkRKdQmp0KpGhkYBNqtV11US4Ilp9oVcnRkRWG2Myj16uY/EpFcSKvfZ+RIgjBKfDyab8TSzds5SVeSuJcccwNGkoAxMHUuQtYmfxTnYW7ySnJIeckhwKva2dof5IDX+9+4wPn/ERHxZP/4T+ZKVm0SOqR2NNICo0inCXrR0A1PvqqTf1RIdGEx8eT3RoNJ4aD8VeW5sKCwkj3BV+RPNUjDuGqNCotvzKvibEEUJIaNe9VPp8sGsXrFsHpaWHlw8aBOPHg8vVuuMYA8XFcOiQfd9g8GBwtFOFq+t+60qdAry1XvaU7iG/Mp9IVyTR7mh8xsfWwq1sKdhCbllu403vWl8tTnHidDg5WH6Q7EPZ7C/f3+xxhyQNoaKmgrnrD0+w63a6SY9Lp298X8b3HE/v2N7Eh8cT644l3BVORU0FZdVl1Ppq6RHVg5SoFBIjEglxhOAQB+Eh4SRFJLXbDfPOYONGWLEC0tNh4EBITYWGSlVZGWzbZl8OB5x2GvTubdft2mX383ggPNy+KiuhqMhe9NPSYNQoGDDAbvf227B0qd03IgKioqBnT7tdXBzs3w+5uVBefuRyr9e+iors+r17YetWe97mREbCmWdCbKyNp7LSxlNUBCUlNnmFh0NICBw4YI99NK8XwsLa9ntuoAlKqQCoqquivKac6rpqSqpK+HDXh7y19S2W7LZjujbcBzheLabh5n+EKwKXw0W9qafeV09CeALn9j+XEd1GkBieSK2vljpfHX3j+pKVlkV8eDwAnmoPO4p3kBieSGpMarvce6iuhh07YPt26NsXRow4fFEHqKmBffvsBfXAAXvRTEyEhAR7cY6IsBf/11+H//zH1gSGDLEX9OHDoVcve4Hu3Rt69LDHNgY2bICFC+3P3Fz7ioqyiWXgQAgNtRfXmhqYOBHOOw/cbqiogH/9C155Bbp1szWN8HD4979h9eojyyZyuCw+39fLnpIC9fW21nEiwsNh8mQbj9dry795M+Tl2eOFh9syR0XBmjVw8OCRtZrY2MPfS1aW/a5GjbLlAXuMtWth8WJYsgRycuz3HB5uE97IkTbh1dba89fW2u82Lc0ew+k8fK7W1sC+Cb0HpVQbqqmvaaxpFHmLKPIWcaD8APs8+8gty2Vb0TY2F2xmd8luzFGPBQ5JGsK5/c4lLCQMb52X2vpaUmNS6RPbh26R3Rq7GhsMAxMGMjhpMAnhCS1E0jYOHLAX+XXr7EV91Cjo3t3WJL76yl7YKivtRcznsxe4iAi7rCEp5OYeefEcNAi+8x2bCJYts8epq2tdPBkZcMYZtlbw1VdQeFT+joqyxy8ogD177LJevewrNdXWJLZutXH7fLZm4HTaJJqQAOecA++9Z2sRQ4bYcu3ZY+MfOxZ+8AM491ybKLZutT8bREbacw8caJPe8uX25XTaJJGVBcnJh7+viAibiGNiYPduW56tW+13PGVK87WS+npba4qJ+XqSLy+3339Y2JHruoKW7kFpglLqOAorC9lUsAlvrZfqelvjySnJYVfxLnI9uRwoP8DB8oMUeYuo9dW2eJwYdwz94/szJGkIgxNtcmm4r5KVlsWAhAFf28fnsxetrVvtRSkx0SaI5OTDF6HqavvXsNcL48ZBdLS94L//Prz0kr1YN9RIamrsRb242B5j4EBbqzl0yDZN7dp1OFmUlNjEBPYv+eqjOsWJ2L+2IyPthdHhONzE5Hbbv7Z79YI+feyFu18/e7wFC+xf7mFhMGGCbQobONBu27374aavoqLDF3OHA779bXucBsbYsjTUvnbutGXYutVe/GfMsLWi1GbGr6n1/zO5XLa8H3wAL7wA775rk8PPf25rVSJQVWW/r5SUVv26qG9AE5RSLfAZH9uLtrNy30rWHVxHRW0FNfU1FFcVs2b/GnYW72x2v5SoFHrF9qJHVA+6R3YnMTyRqNCoxpv3jsoe7N/ci9jQBEYOjKd/7wi2boWPPoLPP7cX5G99y16g16619x0+/dReuBMT7cXzyy9tojhadLS9WIeE2G1qauxyEduEduiQbfZJSLCJoajIXszdbrssLs5u01CTANt006/f4b/cQ0PhrLPg/PNtzWXfPvtX/sGDMGyYbQaKjPxm33nDX/tNm4rUqUsTlDol1dTXsCl/E2v2r+Grg19hjCExIpFYdyy7Snax9sBa1h5YS2m17d4U6gxtfDAzMjSS0T1GMyYpix6+8Qzo7yDcZZ9p6R3bm3BX+BHnMgY++8zev/jgA/sXfXMcDnvB37PHJo4GCQk2YYFNJpWVtrknK8veaykvt8v37z98M76qytZCsrJsrWHFCtusFBkJV11laxChoS1/P9XV9kZ6crK9b6FUIGiCUkGlzlfHp7s/ZXvRdjvsS1giJZ4a1u/bzsa8HPbVrWePdyP7PPvwGVtFiHBFEOIIoay6DLxxhEfWM6rncEZ1H8X4nuMZnTyBqtyh+Ops36GCAnjtNXjzTds9NzHRNv8MH25rEbm59uZ1WJhNDuvW2SayyEh7n+L00w8njtxcWwPp3RsmTbLJwOez+3zxha31nHaa1ijUqUkTlOqy1h/M5p2Nn+ALKafWV8O2gp288ckeyjaPhbxMKBwERQOgJvrwTlJPXO999Ms4yKiRMGlsD84Y2ZPPP3Pywgs+Pv7YQUyMYfJkYcIE20z2/vu2ltJUXBxcfLGtpaxYYZvn9u6FpCTbRBcba2sxlZX2HsWVV9oOAFHt+2iOUkFFE5Tq9CpqKlizfw3e2ioK8p18sbqal18v48CacVA8ABy1EF4EdeFQHQNA7/RaevXz0r13Kd1S6khP6kZSbCR799oeYitWHPlwIthnTWbPhvx8m3C2bbM39M8/39Z8Gpq6wsLsg4xNm8iMsTfYj9VsppQ6MTqShOp0jDH8+4Pt/O2ZIrbklJNf4MOUJ0LRKKixCUhcVQzNzGPm2WWYmnBKi5NxOh1MmgRTp0KPHi7s0JAxLZzjcJfgnTvtzf2srCO74ZaU2KTUmq65IpqclOoomqBUuysqgk8+gQ8W13CovABvyD4O1exh7UcDqN07CpxVhCYconsCpA0Oo096Ob37FTJkKFx5XjqRkf2+8blFbDfj1FSb0JoTF/eND6+UakeaoFS7qKiA+fPh6adhxQqDMQKuOnBEQPV4YDzRaXv43p2fc/8t/RjSq3egQ1ZKdTKaoFSbqqmBBx+Ev/7VUFYmdE8vJOrcZ/Ckvs4lZ/fm2nFXMSwxgxjSSEjojYgmJqVU8zRBqZPmMz5W5a3itc/W8497plC8sz8Mfxkm/JWDvT9jQtoE5pw7h4m9JwY6VKVUF6IJSn0jxhgW5yzmn8tfZuGSQ5RsGQGf3YkjpJ7T7/hfZl5Yz6gev2Jkt5GkxaTpPDpKqROmCUq1mjGwZInhLy/sZfGqXIr2pULR42Ds06UTJ9fw0r9C6d37zgBHqpQKBpqg1HEZA0+/vI/f/d7HnvW9ICSJkORyRo9yct4ZhjPPsA+yJiZq/2ulVNtpVYISkenAY4ATeNoY8/BR638JXNnkmEOBZGNMkYjkAB6gHqhr7mEs1fkYY1h/aD3PLP6Ifz6QRfmWLIjdTb8r53Drj6O5Mev7jTOhKqVUezhughIRJ/A34BwgF1gpIm8aYzY2bGOMeQR4xL/9TODnxpgmw2Ay1RhT0KaRqzZhjB1PLiYGwLB0z1Je3/w6r2YvJOejb8H7f8TpFL57+4f8792D6Jf0i0CHrJQ6RbSmBjUB2G6M2QkgIvOBi4CNLWx/OTCvbcJT7WH/fvj1r+1MnNu22Unc4hKrIW05JZErkP0XILl/gNpQpp5dzXP/dNO797RAh62UOsW0JkGlAnubfM4FTmtuQxGJAKYDtzRZbID3RMQA/zDGPNnCvjcCNwL07q3PxrSXV16BG2+0k8CNP8PL2BnbyKn/jN3bwwnZfyZScAcjRxqm3eLk29+Gc891d7nZOZVSwaE1Caq5y1NLI8zOBD47qnlvojEmT0S6Ae+LyGZjzJKvHdAmrifBDhbbirhUM4yxD8quWWMHQE1Ls9M6FBXBli2GhQuFnoNzib/sRyxxLARgQMIA/nrnbVw/thch4tApH5RSnUJrElQu0KvJ5zQgr4VtZ3NU854xJs//85CIvIZtMvxaglJt46674JFH7BTan312eEK88Mg66sIOwJSn2D/5Yc7qdwY/GTSH8wedz6DEQcc+qFJKBUBrEtRKYKCI9AX2YZPQFUdvJCKxwFnAVU2WRQIOY4zH//5c4LdtEbj6uj/9ySann/wE/vIXO1DqoZJyHlzya/7+5aP0ievD3RPv5jtD99Itslugw1VKqWM6boIyxtSJyC3Au9hu5s8YYzaIyI/965/wb/od4D1jTEWT3bsDr/lHEQgBXjLGLGrLAijrhRfgl7+ESy+Fxx6zyWnZ3mVc+eqV5JTkcMuEW/j9tN8TFaoz6SmlugadsDAILF5sJ9o76yx4+21wuup4aMlD/M+S/6FXbC9euPgFJvWZFOgwlVKqWTphYZDasgUuuQQGDYIFCwBnNefNncn7O9/nqoyr+OuMvxIbFhvoMJVS6oRpgurCCgrsNOUuF7z1FsTE+rjy1Wt4f+f7PHnBk9ww7oZAh6iUUt+YJqgu7OabITcXPv4Y+vaFO9+/m/nZ8/nj2X/U5KSU6vIcgQ5AfTNbttgmvdtvh6ws+NsXf+ORzx/hJ+N/wi/P+GWgw1NKqZOmCaqL+t//Bbcbbr0VVuWt4ufv/pyZg2by2PTHdO4lpVRQ0ATVBeXmwr/+BddfD+GxHi5/5XK6R3XnuYufw+nQYSCUUsFB70F1QXPm2OGLbr8dfvrOT9lZvJPFVy8mITwh0KEppVSb0RpUF1NYCE8+CVdcAZ97XuL5r57nvkn3MbnP5ECHppRSbUprUF2IMXDPPVBRAd+/KY9Zb9/EGb3O4P6z7g90aEop1ea0BtVFGAN33AFPPQW33+Hjd1suxxjDi995kRCH/p2hlAo+mqC6AGPgV7+y955uuQWSLnyEJbuX8JcZf6FvfN9Ah6eUUu1C//Tu5IyBe++Fhx+GH/0IrrhzOWc9dz+zhs3iB6N+EOjwlFKq3WgNqhPz+exzTn/4A1x/g4+0y3/PWc9NpkdUD544/wl93kkpFdS0BtVJ1dfDDTfAs8/CdTeXsHbUuTz98UouHX4pfzvvbyRGJAY6RKWUaleaoDqh2lq46ir497/hp3cW8Wr3UVQWV/DyrJe5dPilgQ5PKaU6hCaoTqaqyk46+N//wt0PFjI3egxVtV4WX72YUT1GBTo8pZTqMHoPqhMpLoYLLrDJ6VcP5zI3egwVtRV8+IMPNTkppU45mqA6AWPgxRdh8GD4+GPD9Dvm8cfqdKrrqzU5KaVOWa1q4hOR6cBjgBN42hjz8FHrpwBvALv8i141xvy2NfsGo8pKWLsW1q2D9ettAkpIgORk23yXkmK3212ymzfWfM7/3j6afWuHEtt/E/Hf/ymLoj7khjE38PDZD+v4ekqpU9ZxE5SIOIG/AecAucBKEXnTGLPxqE0/NcZc8A337ZRqauxstS315vb5oLwcvF6blJYvt3M0vfOOXQYQHe0jxOWjtMSJzyf85iEPp931W7aGLCBnTw288D6UpNP9e/9Dn7PfoXt0EndNXMrE3hM7rqBKKdUJtaYGNQHYbozZCSAi84GLgNYkmZPZt8MZA8uW2XtAb79taz8iEB4OkZG2FpSYCA4H7NtnXzU1Rx4jIqEE9/i3qOv1OrXJK/HE7gEBfAIHRlM6703ev+9+xl3bB88b38frjeKt94WpU+4HdEw9pZRq0JoElQrsbfI5Fzitme1OF5GvgDzgDmPMhhPYFxG5EbgRoHfv3q0Iq23l5tqRGhYuhJAQOPNM+PWvbdLyesHjgaIi+6qthdNPh7Q0iI6v4rP97/NR7tvUxK0npP9Gzux7JkOThpIUMYHE8ETiwuKICo2yr9vD+f6sGFY9fgvx8bD4I5gwocOLq5RSnV5rElRzDVzmqM9rgD7GmHIROQ94HRjYyn3tQmOeBJ4EyMzMbHab9mAMPPMM/OIXUFdnx7u79lqIjf36thvzN/KHpX9gRe4KqsMTKI1IYsW+FRQkFHDZpMv45Rl/ZnSP0cedNPDTT+3QRVdcASNHtlPBlFInpba2ltzcXKqqqgIdStAICwsjLS0Nl8vVqu1bk6BygV5NPqdha0mNjDFlTd4vFJHHRSSpNfsG2pw5dpTwqVPh6aehXz+7vLqumu1F29lTuoe9ZXt5d8e7vLrpVSJcEUwfMB1PtYc8Tx5ZaVk8cNYDjOs5rtXnjI21wxcppTqv3NxcoqOjSU9P12HF2oAxhsLCQnJzc+nbt3WDXLcmQa0EBopIX2AfMBu4oukGItIDOGiMMSIyAdt9vRAoOd6+gbRwIfzylzBrFrz8MizL/Yxfv/p31h5Yy+aCzdSb+sZtY92x3DfpPm7NupWkiKQARq2U6ghVVVWanNqQiJCYmEh+fn6r9zlugjLG1InILcC72K7izxhjNojIj/3rnwBmATeJSB3gBWYbYwzQ7L4nWrD2sHEjzJ4NY8bAbQ9/yQXz7uWd7e+QGJ7I6b1O56LBFzG823D6xPahV2wvekb31HmXlDrFaHJqWyf6fbbqimuMWQgsPGrZE03e/xX4a2v3DbQ9e2DmTNsz79ZHP2LSi2cTHx7PH8/+I7dMuIUIV0SgQ1RKqVPeKTeSxNq1kJUFhYXwz5cKuWP5bEZ2H8nOn+3kzol3anJSSnUKJSUlPP744ye833nnnUdJSUnbBxQAp1SCeu89mDTJdiNf8qmPv+RdhafGw7zvziM2rJlue0opFSAtJaj6+vpmtj5s4cKFxMXFtVNUHeuUSVDZ2bZZr39/+zDu4oq/sGj7Iv7v3P9jWPKwQIenlFJHuPvuu9mxYwejR49m/PjxTJ06lSuuuIKR/mdTLr74YsaNG8fw4cN58sknG/dLT0+noKCAnJwchg4dyg033MDw4cM599xz8TYMcdNFnBJ3/evr4brrICYG3n8fvix7jzs/uJOZg2ZyU+ZNgQ5PKdXJ3bboNtYeWNumxxzdYzSPTn+0xfUPP/ww2dnZrF27lo8//pjzzz+f7Ozsxi7azzzzDAkJCXi9XsaPH893v/tdEhOPnMh027ZtzJs3j6eeeopLL72UV155hauuuqpNy9GeTokE9dhj8MUXMG8erC5dxMXzL2Zo0lCevehZ7aWjlOoSJkyYcMTzQ3/+85957bXXANi7dy/btm37WoLq27cvo0ePBmDcuHHk5OR0VLhtIugT1PbtcN99tnkvaszbXDT/EkZ0G8H7339fRwpXSrXKsWo6HSUyMrLx/ccff8wHH3zAsmXLiIiIYMqUKc2OeOF2uxvfO51ObeLrbG6+2Y5I/qfHKhj90vfI6J7Be1e9R3x4fKBDU0qpFkVHR+PxeJpdV1paSnx8PBEREWzevJnly5d3cHQdI6gTVFUVfPAB3HMP7JdVeOu8PDjlQU1OSqlOLzExkYkTJzJixAjCw8Pp3r1747rp06fzxBNPkJGRweDBg8nKygpgpO0nqBPUjh12MNjhw2Fl3koAxvccH+ColFKqdV566aVml7vdbt55551m1zXcZ0pKSiI7O7tx+R133NHm8bW3oO5mvm2b/TlwoE1Q6XHpJEcmBzYopZRSrXLKJKgv9n2htSellOpCgj5BJSVBrSufnJIcTVBKKdWFBH2CGjQIVuWtAmB8qiYopZTqKoI+QTXcfxKEcSmtn1RQKaVUYAVtgqqshH37DieoIUlDiHZHBzospZRSrRS0CWr7dvtzwADDyn0rtXlPKRX0oqKiAMjLy2PWrFnNbjNlyhRWrVp1zOM8+uijVFZWNn4O1BQeQZugtm61P2NSDnKw4qB2kFBKnTJ69uzJggULvvH+RyeoQE3h0aoEJSLTRWSLiGwXkbubWX+liKzzvz4XkVFN1uWIyHoRWSsix07bbaihi3lRxBcATEid0FGnVkqpNnHXXXcdMSfUAw88wIMPPsi0adMYO3YsI0eO5I033vjafjk5OYwYMQIAr9fL7NmzycjI4LLLLjtiPL6bbrqJzMxMhg8fzm9+8xvADkKbl5fH1KlTmTp1KnB4Cg+AOXPmMGLECEaMGMGjjz7aeL72mNrjuCNJiIgT+BtwDpALrBSRN40xG5tstgs4yxhTLCIzgCeB05qsn2qMKTjpaE/Atm3QowesL/kcl8PFqO6jjr+TUko147bb7GzcbWn0aPBf31s0e/ZsbrvtNm6++WYA/v3vf7No0SJ+/vOfExMTQ0FBAVlZWVx44YUtzszw97//nYiICNatW8e6desYO3Zs47qHHnqIhIQE6uvrmTZtGuvWreNnP/sZc+bMYfHixSQlJR1xrNWrV/Pss8+yYsUKjDGcdtppnHXWWcTHx7fL1B6tqUFNALYbY3YaY2qA+cBFTTcwxnxujCn2f1wOpJ1UVG2gaQ++jO4ZuEPcx99JKaU6kTFjxnDo0CHy8vL46quviI+PJyUlhV/96ldkZGRw9tlns2/fPg4ePNjiMZYsWdKYKDIyMsjIyGhc9+9//5uxY8cyZswYNmzYwMaNG1s6DABLly7lO9/5DpGRkURFRXHJJZfw6aefAu0ztUdrxuJLBfY2+ZzLkbWjo10HNB0kygDviYgB/mGMebK5nUTkRuBGgN69e7cirGPbtg2mz6jn1X0ruSqj60zQpZTqfI5X02lPs2bNYsGCBRw4cIDZs2czd+5c8vPzWb16NS6Xi/T09Gan2miqudrVrl27+NOf/sTKlSuJj4/nmmuuOe5xjDEtrmuPqT1aU4Nqrt7YbJQiMhWboO5qsniiMWYsMAP4iYhMbm5fY8yTxphMY0xmcvLJjZdXVgYHD4IvYQueGg8XDb7o+DsppVQnNHv2bObPn8+CBQuYNWsWpaWldOvWDZfLxeLFi9m9e/cx9588eTJz584FIDs7m3Xr1gFQVlZGZGQksbGxHDx48IjBZ1ua6mPy5Mm8/vrrVFZWUlFRwWuvvcakSZPasLRHak0NKhfo1eRzGpB39EYikgE8DcwwxhQ2LDfG5Pl/HhKR17BNhktOJujjaehivkPeITkimWn9prXn6ZRSqt0MHz4cj8dDamoqKSkpXHnllcycOZPMzExGjx7NkCFDjrn/TTfdxA9/+EMyMjIYPXo0EybYDmOjRo1izJgxDB8+nH79+jFx4sTGfW688UZmzJhBSkoKixcvblw+duxYrrnmmsZjXH/99YwZM6bdZuqVY1XZAEQkBNgKTAP2ASuBK4wxG5ps0xv4CPiBMebzJssjAYcxxuN//z7wW2PMomOdMzMz0xyvn/6xvPwyzJ4NobeM54YZp/HX8/76jY+llDo1bdq0iaFDhwY6jKDT3PcqIquNMZlHb3vcGpQxpk5EbgHeBZzAM8aYDSLyY//6J4BfA4nA4/62zjr/yboDr/mXhQAvHS85tYWGLuY1MRu4YuRj7X06pZRS7aBVExYaYxYCC49a9kST99cD1zez306gw/t3b9sG7vgCeiR34/S00zv69EoppdpAUI4ksXFLLdWx2Vw+4vIWnw1QSqnjOd4tEHViTvT7DMoEle8pgqRNXDHyikCHopTqosLCwigsLNQk1UaMMRQWFhIWFtbqfVrVxNfVpN3+XSK9JYzsflOgQ1FKdVFpaWnk5uaSn58f6FCCRlhYGGlprR/HIegSlM/4OD3tdPon9A90KEqpLszlctG3b99Ah3FKC7oE5RAHj5z7SKDDUEopdZKC8h6UUkqprk8TlFJKqU7puCNJBIKI5APHHmDq+JKADp3iI0C0nMHnVCmrljO4nEw5+xhjvjYIa6dMUG1BRFY1N3RGsNFyBp9TpaxazuDSHuXUJj6llFKdkiYopZRSnVIwJ6hmJ0YMQlrO4HOqlFXLGVzavJxBew9KKaVU1xbMNSillFJdmCYopZRSnVLQJSgRmS4iW0Rku4jcHeh42oqI9BKRxSKySUQ2iMit/uUJIvK+iGzz/4wPdKxtQUScIvKliLzl/xys5YwTkQUistn/b3t6MJZVRH7u/73NFpF5IhIWLOUUkWdE5JCIZDdZ1mLZROQe//Vpi4h8OzBRn7gWyvmI/3d3nYi8JiJxTdaddDmDKkGJiBP4GzADGAZcLiLDAhtVm6kDbjfGDAWygJ/4y3Y38KExZiDwof9zMLgV2NTkc7CW8zFgkTFmCHZyz00EWVlFJBX4GZBpjBmBnZl7NsFTzueA6Ucta7Zs/v+zs4Hh/n0e91+3uoLn+Ho53wdGGGMygK3APdB25QyqBAVMALYbY3YaY2qA+cBFAY6pTRhj9htj1vjfe7AXslRs+Z73b/Y8cHFAAmxDIpIGnA883WRxMJYzBpgM/BPAGFNjjCkhCMuKHZg6XERCgAggjyAppzFmCVB01OKWynYRMN8YU22M2QVsx163Or3mymmMec8YU+f/uBxomEujTcoZbAkqFdjb5HOuf1lQEZF0YAywAuhujNkPNokB3QIYWlt5FLgT8DVZFozl7AfkA8/6mzOfFpFIgqysxph9wJ+APcB+oNQY8x5BVs6jtFS2YL5GXQu843/fJuUMtgTV3PzuQdWPXkSigFeA24wxZYGOp62JyAXAIWPM6kDH0gFCgLHA340xY4AKum4zV4v8918uAvoCPYFIEbkqsFEFTFBeo0TkXuxtiLkNi5rZ7ITLGWwJKhfo1eRzGrYpISiIiAubnOYaY171Lz4oIin+9SnAoUDF10YmAheKSA62ifZbIvIiwVdOsL+vucaYFf7PC7AJK9jKejawyxiTb4ypBV4FziD4ytlUS2ULumuUiFwNXABcaQ4/WNsm5Qy2BLUSGCgifUUkFHuT7s0Ax9QmRESw9yo2GWPmNFn1JnC1//3VwBsdHVtbMsbcY4xJM8akY//9PjLGXEWQlRPAGHMA2Csig/2LpgEbCb6y7gGyRCTC/3s8DXsPNdjK2VRLZXsTmC0ibhHpCwwEvghAfG1CRKYDdwEXGmMqm6xqm3IaY4LqBZyH7U2yA7g30PG0YbnOxFaR1wFr/a/zgERsL6Ft/p8JgY61Dcs8BXjL/z4oywmMBlb5/11fB+KDsazAg8BmIBv4F+AOlnIC87D31mqxNYfrjlU24F7/9WkLMCPQ8Z9kObdj7zU1XJOeaMty6lBHSimlOqVga+JTSikVJDRBKaWU6pQ0QSmllOqUNEEppZTqlDRBKaWU6pQ0QSmllOqUNEEppZTqlDRBKaWU6pQ0QSmllOqUNEEppZTqlDRBKaWU6pQ0QSmllOqUNEEppZTqlDRBKdVORCRHRM4OdBxKdVWaoJRSSnVKmqCU6kD+GUYfFZE8/+tREXH71yWJyFsiUiIiRSLyqYg4/OvuEpF9IuIRkS0iMi2wJVGq/YUEOgClTjH3AlnYmXQNdirw+4D7gduxM5Um+7fNAox/SvhbgPHGmDwRSQecHRu2Uh1Pa1BKdawrgd8aYw4ZY/KxU6F/37+uFkgB+hhjao0xnxo75XU9dor0YSLiMsbkGGN2BCR6pTqQJiilOlZPYHeTz7v9ywAeAbYD74nIThG5G8AYsx24DXgAOCQi80WkJ0oFOU1QSnWsPKBPk8+9/cswxniMMbcbY/oBM4FfNNxrMsa8ZIw507+vAf7YsWEr1fE0QSnVvlwiEtbwAuYB94lIsogkAb8GXgQQkQtEZICICFCGbdqrF5HBIvItf2eKKsDrX6dUUNMEpVT7WohNKA2vMGAVsA5YD6wBfuffdiDwAVAOLAMeN8Z8jL3/9DBQABwAugG/6rASKBUgYu/BKqWUUp2L1qCUUkp1SpqglFJKdUqaoJRSSnVKmqCUUkp1Sp1yqKOkpCSTnp4e6DCUUkp1gNWrVxcYY5KPXt4pE1R6ejqrVq0KdBhKKaU6gIjsbm65NvEppZTqlIIuQfmMj5fWv8Si7YsCHYpSSqmT0Cmb+E6GIPxuye9Ijkxm+oDpgQ5HKaXUNxR8CUqEK0Zewf2L72dP6R56x/YOdEhKqS6otraW3NxcqqqqAh1K0AgLCyMtLQ2Xy9Wq7YMuQQGNCWp+9nzunHhnoMNRSnVBubm5REdHk56ejh2/V50MYwyFhYXk5ubSt2/fVu0TdPegAPrF9yMrLYuX1r8U6FCUUl1UVVUViYmJmpzaiIiQmJh4QjXSoEtQxsD//A8M2PNbvjr4FRsObQh0SEqpLkqTU9s60e8z6BKUCPz3v7Bh4RSc4tRalFJKdVFBl6AALrkEvlzlYmLsbF7KfgmdUkQp1dWUlJTw+OOPn/B+5513HiUlJW0fUAAEbYIC6L3/Z+SU5LA8d3lgA1JKqRPUUoKqrz/2ZMoLFy4kLi6unaLqWEGZoAYNguHDYfeysYSHhDN3/dxAh6SUUifk7rvvZseOHYwePZrx48czdepUrrjiCkaOHAnAxRdfzLhx4xg+fDhPPvlk437p6ekUFBSQk5PD0KFDueGGGxg+fDjnnnsuXq83UMX5RoKymznYWtRDD4Vw/g8uZ8HGBTw2/TGcDmegw1JKdUG3LbqNtQfWtukxR/cYzaPTH21x/cMPP0x2djZr167l448/5vzzzyc7O7uxi/YzzzxDQkICXq+X8ePH893vfpfExMQjjrFt2zbmzZvHU089xaWXXsorr7zCVVdd1ablaE9BWYMCm6B8PkjLu5mDFQf5ZPcngQ5JKaW+sQkTJhzx/NCf//xnRo0aRVZWFnv37mXbtm1f26dv376MHj0agHHjxpGTk9NB0baNoK1BjRoFffvCzmWjiZoUxfzs+Xyr77cCHZZSqgs6Vk2no0RGRja+//jjj/nggw9YtmwZERERTJkypdnni9xud+N7p9PZ5Zr4grYGJWJrUYs/dDIjbTavbHqFmvqaQIellFKtEh0djcfjaXZdaWkp8fHxREREsHnzZpYvD86OYEGboMAmqJoa6H3oJoq8RXyw84NAh6SUUq2SmJjIxIkTGTFiBL/85S+PWDd9+nTq6urIyMjg/vvvJysrK0BRti/pjM8IZWZmmraYsNDng/R06NfPx1czErlw8IU8f/HzJx+gUirobdq0iaFDhwY6jKDT3PcqIquNMZlHbxvUNSiHA267DT75xMEkx+28tuk1qup0ZGKllOoKgjpBAdxwA8TFQfGHN+Cp8TA/e36gQ1JKKdUKQZ+goqPh5pvhs/e6McxxMTf89wae/fLZQIellFLqOII+QQH87GcQGiqMz3mZqelTufbNa/nN4t/oGH1KKdWJnRIJqnt3+OEPYd6LoTw99W1+OPqH/HbJb/nxWz+m3nfsca2UUkoFximRoADuuAPq6+HiC13cPfSf3HPmPTy55kmufv1q6nx1gQ5PKaXUUU6ZBNW/P7z2GuTkwLhxwoiDv+ehbz3E3PVzufQ/l2rvPqVUlxcVFQVAXl4es2bNanabKVOmcLzHeB599FEqKysbPwdqCo9TJkEBzJwJa9faYZCuvBIi1/yKx6Y/xmubX2Pq81PZ79kf6BCVUuqk9ezZkwULFnzj/Y9OUIGawuOUSlAAvXvDxx/DRRfZZr+xtT9jwfcWsO7gOsY/NZ5VeSf/gLBSSrWFu+6664g5oR544AEefPBBpk2bxtixYxk5ciRvvPHG1/bLyclhxIgRAHi9XmbPnk1GRgaXXXbZEePx3XTTTWRmZjJ8+HB+85vfAHYQ2ry8PKZOncrUqVOBw1N4AMyZM4cRI0YwYsQIHn300cbztcfUHkE9ksSxlJZCZiZUVMCaNXCQr7hw/oUcKD/AOf3O4aLBF3HBoAtIiU5p1ziUUp1T0xEPbrvNtr60pdGjwX99b9GXX37Jbbfdxief2NkYhg0bxqJFi4iLiyMmJoaCggKysrLYtm0bIkJUVBTl5eXk5ORwwQUXkJ2dzZw5c8jOzuaZZ55h3bp1jB07luXLl5OZmUlRUREJCQnU19czbdo0/vznP5ORkUF6ejqrVq0iKSkJoPHz7t27ueaaa1i+fDnGGE477TRefPFF4uPjGTBgAKtWrWL06NFceumlXHjhhc1O7aEjSbRCbCy88gqUlMBll4EcGsWSq1Zyc+bNbMjfwI1v3UjPOT0Z+reh/PitH/PS+pfYVrhNu6YrpTrMmDFjOHToEHl5eXz11VfEx8eTkpLCr371KzIyMjj77LPZt28fBw8ebPEYS5YsaUwUGRkZZGRkNK7797//zdixYxkzZgwbNmxg48aNx4xn6dKlfOc73yEyMpKoqCguueQSPv30U6B9pvYI2uk2WiMjA558Er7/fXtfSqQbAwf+P3587RwmXbGJpYfe4pPdnzAvex7/WP0PAOLC4piQOoFpfacxre80RvcYrRMhKhXkjlfTaU+zZs1iwYIFHDhwgNmzZzN37lzy8/NZvXo1LpeL9PT0ZqfaaEpEvrZs165d/OlPf2LlypXEx8dzzTXXHPc4x/oDvT2m9jhla1ANrroKNm+G+fPhN7+Bnj3h7ruFb48dxu6X7uQqeZvPLyxi9fVreXrm01w2/DJyy3K564O7yHwqk6RHkrhw3oX86fM/sXTPUkqqSgJdJKVUEJk9ezbz589nwYIFzJo1i9LSUrp164bL5WLx4sXs3r37mPtPnjyZuXPnApCdnc26desAKCsrIzIyktjYWA4ePMg777zTuE9LU31MnjyZ119/ncrKSioqKnjttdeYNGlSG5b2SKd0DarB4MH2BTZJrV0L//d/8PTTYO9POomLG8XMmaO45JLr+L+rocy3nw93fcgnOZ+wZM8S/rv1v43H6xXTi4m9J3LhoAuZMXAGcWFxASiVUioYDB8+HI/HQ2pqKikpKVx55ZXMnDmTzMxMRo8ezZAhQ465/0033cQPf/hDMjIyGD16NBMmTABg1KhRjBkzhuHDh9OvXz8mTpzYuM+NN97IjBkzSElJYfHixY3Lx44dyzXXXNN4jOuvv54xY8a020y9p2wnidaoqYFNm+DLL23PvzffhOJiuy45GXr1grQ0+zM22UN0/w3Q5xPWHfqKD3d9yKGKQ4Q4QhjTYwzjUsYxNmUs43qOY3jycNwh7mOeWykVWDrdRvs4kU4SWoM6htBQe29q1Ci45hqorYVPPoFlyyA3F/buhZ074dNPobg4GsgiIyOLn/8cnvhRPRtKvuC/W//L8tzlvLRyIU/syAHPMhzViXRz92HazHxmZg1jUp9J9IzuGeDSKqVU56IJ6gS4XHD22fZ1NI8H/vMf+H//z477d+21TtLTT2fo0NMpzAXPegPG3qj0AQeAuYvKmXvOHZA5mx7RPRjTYwyZPTO5ZOgljOo+qtkbm0opdao45TtJtJXoaLj2Wli3Dj76CB58ECZMsDWtbt3ggQeEjz6CXbts1/bdu+HsyRHw9hMMXpTDiIqfsbd4P7//9PeM+ccYRvx9BH9c+kfKa8oDXTSlTlmd8RZIV3ai36fegwogY+CJJ+DOO6G8HJKSYPoFVaRMeofPmcNne5fSI6oHD33rIa4edbV2Z1eqA+3atYvo6GgSExO1NaMNGGMoLCzE4/HQt2/fI9a1dA9KE1QnUFEB775rHxx+802brDIyYPrlO1mScDXL9y9lXMo4XrvsNXrF9gp0uEqdEmpra8nNzT3us0Gq9cLCwkhLS8Plch2xXBNUF+HxwLx5tmb15ZcwYYLh0nsX8tuNVxDjjmHRlYsY3m14oMNUSqk2o0MddRHR0XDjjbB6tX14ePt24b7Lzuem+o3U++o589kzWbpnaaDDVEqpdqcJqpMSsWMEZmfDtGnwx/tSubYqm+6R3fn2i99mdd7qQIeolFLtShNUJ5eSYu9Lfe978NB9CdwauZzkiGRmzpvJ3tK9gQ5PKaXaTbsnKBF5RkQOiUh2e58rWDkc8MILcNZZcOuP4ri/12Iqaiu4YN4FeKq/Pl6WUkoFg46oQT0HTO+A8wS1sDB4/XUYNAjuuL4vT33rdTYc2sDsV2ZT76sPdHhKKdXm2j1BGWOWAEXtfZ5TQVycHa2ivBw+emYqf5nxFxZuW8ivF/860KEppVSb6zT3oETkRhFZJSKr8vPzAx1OpzV0KNxyi53HKivkx1w/5np+v/T3LNi4INChKaVUm+qQ56BEJB14yxgzojXbn8rPQbVGSQkMHGiT1XsfVjP1hSmsP7ie5dcvZ0S3Vn3FSinVaehzUEEkLg4eesiOov7Gq25eufQVot3RXPLyJdppQikVNDRBdVHXXQdjxsCtt0JNYU9envUyO4p3cMs7twQ6NKWUahMd0c18HrAMGCwiuSJyXXuf81TgdMKLL0JVFZx3HoyMmcz9k+/nha9e4F9f/SvQ4Sml1EnriF58lxtjUowxLmNMmjHmn+19zlPFsGG26/mOHfCd78AvT7uPyX0mc9PbN7GtcFugw1NKqZOiTXxd3JQp8Oyzdqbfn/4khLmXzMUd4ubcF89lU/6mQIenlFLfmCaoIHDFFXDvvTZRbViWxqIrF+Gt9XL6P0/no10fBTo8pZT6RjRBBYn774chQ+DHP4ZhceNZcf0K0mLS+PaL3+b5tc8HOjyllDphmqCChNsNTz0FOTnw619Dn7g+fHbtZ0xJn8I1b1zDnGVzAh2iUkqdEE1QQeTMM+FHP4JHH4WVKyE2LJa3Ln+L7w37Hre/dzv3fngvnXGCSqWUao4mqCDzxz9Cjx4wdSr87nfgq3Uz77vzuHHsjfx+6e/JfCqTR5c/yoHyA4EOVSmljkkTVJCJjbUjTJx7rr0vNXgw/OH3TmZHPcGfpz2NIPz83Z+TOieV6964ThOVUqrT6pCx+E6UjsXXNj75BO6+G5Yvt59DQuDCC2HWdTl84XiMv638G+4QN/eceQ+/OP0XhIWEBTZgpdQpqaWx+DRBnQKKimyS+ugjeOYZKC6GiRMhc3Ihy6qf5ouafzJokIOnLnySyX0mBzpcpdQpRhOUAuxcUs88A48/Dlu2HF4ePmwx3mk3cN3ZU7gp8ybGpoxFRAIXqFLqlKEJSn1NRQVs3gzvvw8PPWTwVtfjG/8YJiqPaFIZkjyQB27tz3njhwU6VKVUENMEpY4pLw/uvBPmzm2yUHyAj+SsD7jrLsPPLjgbl9MVqBCVUkFK54NSx9Szpx0dvagISkuhvh7Wbylj8qXrKFg9mTsunkF0xmKu/cs/2e/ZH+hwlVKnAE1Q6gjx8RATAw4HjBgYxyfzx3IgN4wrbtmGb08Wz/7sOlKH5nLmD9/m4xVFdMIKuFIqSGgTn2q1igp4+M/5/P3pCgp3pgMQHlvOoEE+MjOimDHDwSWXgPatUEqdCL0HpdrUkuxt/OKvH7JmpQtTMAgpGIapTOTsC4p4+fkEEhICHaFSqqvQe1CqTU0eMZBVT/yYwqWXMO/tfVz23K2EnPNrPlgYTY/+h7juj2+xo2hnoMNUSnVhWoNSbaakqoQ//GcRj92VSfX+AdDjS1LPf46bf5DCLafdTIw7JtAhKqU6IW3iUx2mrg7mPHmAP/0xlPw9CRBxCNeApZwzLYT7rs7i9FHdAh2iUqoT0QSlOlx9Pbz6Kjz3ciEffQRVxYkAhCXv57RJpVx3WXdmzYwnPDzAgSqlAkoTlAooY+DdFbv487yNLPnITcXWCVATg4RW0n/8Nr43M5YfXpLOgAHaC1CpU40mKNVpGGP4Ys9annxlK++9HU7uqnFQlgpAfPdyzj3fyw+vjOScKREUFsLGjfYB4okToZu2DioVdDRBqU6rxFvKnLf/y9OvbGf/2gzYPgPqwpGQKkzdkVOAjB1rJ2McPBgGDLAjYIjYV2Ii2r1dqS5IE5Tq9IwxrD+0nq/2bOfttx2s/iKUXMenVMWvBreHiNyZOHbMoDJnBL665scETEmBESMgIgL274cDB+z7AQOgf39bA4uNheho8Hjg0CE7/chpp8H550NcXMeWWSmlCUp1UbX1tXy+93M+2/sZe0r3sKd0D2vyvuLgPhdSPIgekoFTQhCcuKt64S7MpCK3Hy4i6JMWSs8UB+XlsH077NhhR8NoSgTCw6GyElwumDwZkpMhNBQiI2H4cBgzxv6MidH7Y0q1B01QKmj4jI/Veav579b/siF/A8YY6k09eZ481h1cR019DQAhjhAGJAxgQMIA+sT2oU9sH6KdSVAVi9TGMDilJxMG9ifM5WbFCnjtNfjgA5vEamqgpMS+GjgcNkl16wYjR0JGhq2xFRZCQYFtYrzySujdOyBfi1JdliYodUqora9lY/5Gsg9lszF/IxsLNrKzeCe7S3ZTWl36te2d4mRAwgAGJw1mcOJgBiQMIDE8kRh3DPFhCcTVDmdzdhibN9tkVVpqpyZZv97Wyhr++4SHg9dra1jf+pZtMvR67SskxDYpRkfbZsYxY+zP9ettN/wlS+DSS+FHP7JJsLWMAZ8PnM62+e6UChRNUOqUV1pVSll1GZW1lXhqPGwv2s6GQxvYkL+BrYVb2Va0rbH21cDlcDEmZQyjuo8i1BmKIESGRjI4cTDpkcNJYhD9UmOJjBR27YIXXoDnn4fdu+29r/Bw++Cyx2N/NggNtbU0hwPS02HnTpg0CZ5+2tbK8vJsz8XevQ93BGlw4IA9z9NPQ24uXHCBTXCTJ9vzud02KTbso82SrVdWZpt2NekfqaLCfi/tRROUUsdR76tnn2cfJVUleKo9HCg/wBf7vmD5vuVsyt9EvanHGEN5TTm1vtrG/aJDo0mPSyc1JpVukd1IjkgmPiyB2LAYokOj6RndkwEJA0ly9WL7Nidr1kB2NgwbBhddZO95Pfcc/OIXRzYpNoiIgLQ0m9CqqiA/3z4EPWkSDBkCb7xhO3u0ZMAAuOQS+O53bSeQPXtg715bu/P5bE0sPh66d7fr9+6FLVsgJ8duU11tzxcdbTuYREYe7jkZGwvjxtnelbGxh8/p89lmz337bMwhIfaiX1Jilx04AAMH2tpmzFEjYNXWwrJlsHSp/ex227jOPx969Di83d69sGqVbWItKrLH3bbNvuLi4Ior4PLLj9xn2zZ44gk791lyMsycCWefDWvXwn/+AytW2PMNGGB7io4YYZtyBw6030VJiS1PYqLdPyGh+T8KmmOMTYB799o/LEpK7IW/stJ+/6mp0KsX9O17ZIIsK4NNm6BfP3vOpsvz8+35IyLsd37woP1uw8JsTT0s7GthYMzhRzccDvvyeOz32PD753LZf4fly+Hjj+3vzLBhcO659nEPp9Our6213/GJ1PybowlKqTZS56tjV/EuNhVsYmvhVvaU7mF36W72le0jvzKf/Ip8vHXer+3ndrrpn9CfAQkDGJgwkKSIJKJDo4lxxzCu5zjiaofy1FNCeLi9WDUkk61b7cXX7bYXoh497MV38GB73Pp620yYnW2TSVWVXQb2orV8OXz00ZE1uNZIToaoKFvbczigvNw2cVZU2Itcw6tBt26Hmx3LyuzF63hCQuD0021ydDhs7J98Ys9zNIfDJpPx4+Hdd21yaioqyiaWgQNh1y673uGwF3yXv9Pn5s32nBdeaHtvfvrp4e9l7Fi7vKLCJujNm20zrs/X+u/M4TicvEXs54bvyec7/O9yLNHRkJlp//hYs8aWo2G/7t3tHyt79tjkdCwuly3TyJH296lHD5uY3nzT1vBbIznZ1syHD7e/R0uW2H+jprze5hPhidAEpVQHqq6rxlPjobSqlL1le9lWuI1tRf5X4TZ2FO+gqu7I/+kpUSlM7jOZGHcMguAQR+PLHeKmR1QPUqJS6BPXh9E9RhMVGtXqeIqKYNEie6Hr3dv+pR4Zefgv36IiWwsrKrLrBg60F8rjKSiwF9CVK20SbfiLPDraXhR79rTnqauzCSsuzi7v1s3WWhYtssmzvNxewEUgK8vWls4+2174qqvtBXn+fFvz2b3b3uO7+GK7TffutiYTEXFkDWbTJnjpJdt8WldnX2PGwHXX2WZUsDWGpUth6FB7X/BoXq+9qO/YYRNgXJz9Q6Gw0CaI4mJ7wa6qsjXchkTUNCk1JCoRW1Pq1ct+BwkJ9pjh4YdrgDk5Nil98YWNPyPD1jLHjLHlXr/ebtenj03GPXrYc1dW2vN1726XlZbaWuiyZbbWeOiQXR8WBuecYxNx377298Hns/9eCQk2PjictFNTj/xOvV4bl8NhE6DLZePQGpRSQcQYQ1VdFZ4aD4WVhXy29zM+3PUhy/Yuo7q+GmMMPuPDYKj31VNVV3VErcwhDoYmDaV/Qn9q6muoqqvC5XCRGpNKWnQa6XHpDE0eytCkoUS7o6msraSytpIYdwwRrogAlvzk+Hy2Oappc6I6vtpa2/zXkMg7G01QSnVhxhg8NR7yPHlsL9rO6rzVrMxbyd6yvYSFhBEWEkZVXRX7yvaxv3w/PtNyu1R8WDxpMWlEu6NxOVy4Q9wkhifSPbI7PaJ60Cu2F31i+5AWk0ZYSBgup4sQRwhOceIQR+NnpdpKSwlKf8uU6gJEhBh3DDHuGIYkDeGCQRe0uG2dr47dJbvZXLCZTQWbqKqrIsIVQXhIOKXVpeSW5ZJblktFbQU19TWUVJWwvWg7B8sPUlFb0eJxGzjEQZ/YPgxIGEBKtG0rM8bgdDiJCIkgwhVBtDua+LB44sNtMhyYMJCe0T0R7VKoToAmKKWCTIgjhP4J/emf0J/zB51/Qvt6qj3sLdvL7pLd5HnyqK6vpra+ljpfHT7jw2d8eGo87Czeyfai7Wwp3IIgiAj1vnq8dV4qaiqa7SQS6YokPjz+iNqYQ+zNi+r6aiprK6mpr2Fkt5Gc2ftMxqWMa6w1FnmLSAhPIDkimbiwuMamz6jQKMb1HEfP6J5t8t2pzkUTlFKqUbQ7mmHJwxiWPOykjlPnq6OkqoQibxF7SvewtXArWwu34qn2UGfqGhNew722cFc44SHhCMKaA2t45PNHqPMd7nbY0ITZkrSYNPrF92u81wYQFRrVWOuMc8cRFxaHQxxU11dTU19DrDuW1JhUekb3JDwkHIc4cDqchIeE2xqnK7xxlBKnOOkb3/dr9++MMVorbEd6D0op1elU1layKX8T8eHxpESlEO4Kp6quivyKfEqqShqTSZG3iC/2fcGKfSvI8+QR6YpsTCKeGg+eag9l1WWUVJVQUlWCz/hwh7gJdYZSUlXytQezj0UQ+sT1ITU6lfzKfPZ79lNRW0FcWBzxYfFEhkY2JtxQZyjdIrvRLbIbSRFJJIYnkhiRSIgjhKq6KqrrqgkLCSM2LJYYdwxVdVWUVpVSXlNOYkQivWJ6kRKdglOcjfcTI0MjiXHHEB4S3tgxpunzeKHOUGLdsTgdXe8pY+0koZRSTRhjKKgsIM+TR019DT7jo85Xh7fOS2VtJd5aLyKCU5zU1NewrWgbmwo2sd+zn26R3UiJSiHaHU1JVQnFVcVU1FQ0Nlt667zkV+RzqOIQBZUFrbq31xYEIS4sjqjQKJwOJyGOEHzGR219LTX1NcSFxdEvvh994/oSGxbbGG9FTQWl1TZBRrgiSAhPOOIV446hrLqMIm8RnmoPIvYxCEH46Wk/PelOMwHtJCEi04HHACfwtDHm4Y44r1JKtURESI5MJjky+fgbn6SquioKKwsxGNxON+4Qd2Otqay6DHeIm1h3LFGhURRUFrC3bC/7PfsB2ynFYKioqaCsugxvnbfxGC6Hq7GJsaquimJvMYXeQipqK6j31VPnq8MhDkKdobgcLgq9hews3smy3GVU1FRQb+rxGR8Rrghi3bFEu6OpqKmguKq4san0eG4ef3O79eps9wQlIk7gb8A5QC6wUkTeNMZsbO9zK6VUZxAWEkZqTOoRy2LcMXSL/PoU0fHh8QxMHNhRobWoacIrqy4j1h1LQngC0W77BHdDp5lQZ2i7xdARNagJwHZjzE4AEZkPXARoglJKqU4qLCSMlOiUxkcJAuEkB6holVRgb5PPuf5lRxCRG0VklYisyj/eIFNKKaWCXkckqOb6YH6tZ4Yx5kljTKYxJjM5uf3bhJVSSnVuHdHElwv0avI5Dcg71g6rV68uEJFWjrfboiSg4CSP0RVoOYPPqVJWLWdwOZly9mluYbt3MxeREGArMA3YB6wErjDGbGjn865qrttisNFyBp9TpaxazuDSHuVs9xqUMaZORG4B3sV2M3+mvZOTUkqprq9DnoMyxiwEFnbEuZRSSgWHjugkEShPBjqADqLlDD6nSlm1nMGlzcvZKYc6UkoppYK5BqWUUqoL0wSllFKqUwq6BCUi00Vki4hsF5G7Ax1PWxGRXiKyWEQ2icgGEbnVvzxBRN4XkW3+n/GBjrUtiIhTRL4Ukbf8n4O1nHEiskBENvv/bU8PxrKKyM/9v7fZIjJPRMKCpZwi8oyIHBKR7CbLWiybiNzjvz5tEZFvBybqE9dCOR/x/+6uE5HXRCSuybqTLmdQJagmA9POAIYBl4vIyc281nnUAbcbY4YCWcBP/GW7G/jQGDMQ+ND/ORjcCmxq8jlYy/kYsMgYMwQYhS1zUJVVRFKBnwGZxpgR2MdNZhM85XwOmH7UsmbL5v8/OxsY7t/ncf91qyt4jq+X831ghDEmA/u86z3QduUMqgRFk4FpjTE1QMPAtF2eMWa/MWaN/70HeyFLxZbvef9mzwMXByTANiQiacD5wNNNFgdjOWOAycA/AYwxNcaYEoKwrNhHWsL9D+5HYEeTCYpyGmOWAEVHLW6pbBcB840x1caYXcB27HWr02uunMaY94wxDVMfL8eOFARtVM5gS1CtGpi2qxORdGAMsALobozZDzaJAV8fv7/reRS4E/A1WRaM5ewH5APP+psznxaRSIKsrMaYfcCfgD3AfqDUGPMeQVbOo7RUtmC+Rl0LvON/3yblDLYE1aqBabsyEYkCXgFuM8aUBTqetiYiFwCHjDGrAx1LBwgBxgJ/N8aMASrous1cLfLff7kI6Av0BCJF5KrARhUwQXmNEpF7sbch5jYsamazEy5nsCWoEx6YtisRERc2Oc01xrzqX3xQRFL861OAQ4GKr41MBC4UkRxsE+23RORFgq+cYH9fc40xK/yfF2ATVrCV9WxglzEm3xhTC7wKnEHwlbOplsoWdNcoEbkauAC40hx+sLZNyhlsCWolMFBE+opIKPYm3ZsBjqlNiJ3X+Z/AJmPMnCar3gSu9r+/Gnijo2NrS8aYe4wxacaYdOy/30fGmKsIsnICGGMOAHtFZLB/0TTsRJ7BVtY9QJaIRPh/j6dh76EGWzmbaqlsbwKzRcQtIn2BgcAXAYivTYjIdOAu4EJjTNM54tumnMaYoHoB52F7k+wA7g10PG1YrjOxVeR1wFr/6zwgEdtLaJv/Z0KgY23DMk8B3vK/D8pyAqOBVf5/19eB+GAsK/AgsBnIBv4FuIOlnMA87L21WmzN4bpjlQ2413992gLMCHT8J1nO7dh7TQ3XpCfaspw61JFSSqlOKdia+JRSSgUJTVBKKaU6JU1QSimlOiVNUEoppTolTVBKKaU6JU1QSimlOiVNUEoppTql/w+4hE8ZuJBu3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy and loss plot\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# labels\n",
    "ytest = np.argmax(Ytest, axis=1)\n",
    "\n",
    "# get predictions\n",
    "Ytest_ = model.predict([Xstest, Xqtest])\n",
    "ytest_ = np.argmax(Ytest_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문                |실제값  |예측값\n",
      "---------------------------------------\n",
      "은경이는 어디야 ?          : 복도      복도\n",
      "필웅이는 어디야 ?          : 화장실     화장실\n",
      "경임이는 어디야 ?          : 부엌      부엌\n",
      "경임이는 어디야 ?          : 복도      복도\n",
      "경임이는 어디야 ?          : 부엌      부엌\n",
      "경임이는 어디야 ?          : 복도      복도\n",
      "경임이는 어디야 ?          : 정원      정원\n",
      "수종이는 어디야 ?          : 복도      복도\n",
      "경임이는 어디야 ?          : 사무실     사무실\n",
      "수종이는 어디야 ?          : 사무실     사무실\n",
      "필웅이는 어디야 ?          : 부엌      부엌\n",
      "필웅이는 어디야 ?          : 정원      정원\n",
      "수종이는 어디야 ?          : 사무실     사무실\n",
      "필웅이는 어디야 ?          : 침실      침실\n",
      "필웅이는 어디야 ?          : 침실      침실\n",
      "은경이는 어디야 ?          : 부엌      부엌\n",
      "은경이는 어디야 ?          : 정원      정원\n",
      "은경이는 어디야 ?          : 부엌      부엌\n",
      "수종이는 어디야 ?          : 사무실     사무실\n",
      "은경이는 어디야 ?          : 부엌      복도\n",
      "필웅이는 어디야 ?          : 복도      복도\n",
      "은경이는 어디야 ?          : 사무실     사무실\n",
      "은경이는 어디야 ?          : 사무실     사무실\n",
      "경임이는 어디야 ?          : 복도      복도\n",
      "수종이는 어디야 ?          : 침실      침실\n",
      "경임이는 어디야 ?          : 침실      침실\n",
      "필웅이는 어디야 ?          : 침실      침실\n",
      "수종이는 어디야 ?          : 부엌      부엌\n",
      "수종이는 어디야 ?          : 부엌      부엌\n",
      "수종이는 어디야 ?          : 부엌      사무실\n"
     ]
    }
   ],
   "source": [
    "NUM_DISPLAY = 30\n",
    "\n",
    "print(\"{:18}|{:5}|{}\".format(\"질문\", \"실제값\", \"예측값\"))\n",
    "print(39 * \"-\")\n",
    "\n",
    "for i in range(NUM_DISPLAY):\n",
    "    question = \" \".join([idx2word[x] for x in Xqtest[i].tolist()])\n",
    "    label = idx2word[ytest[i]]\n",
    "    prediction = idx2word[ytest_[i]]\n",
    "    print(\"{:20}: {:7} {}\".format(question, label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatApp_env",
   "language": "python",
   "name": "chat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
